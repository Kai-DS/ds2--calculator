{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'.venv (Python 3.12.7)' でセルを実行するには、 ipykernel パッケージが必要です。\n",
      "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
      "\u001b[1;31mコマンド: '/Users/reo_huk/wednesday4/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "import sqlite3\n",
    "\n",
    "# ランキングページ（2024年版）\n",
    "RANKING_PAGE_URL = \"https://onsen.nifty.com/rank/year/\"\n",
    "OUTPUT_FILE = \"onsen_details.csv\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def fetch_page(url):\n",
    "    \"\"\"指定したURLからHTMLデータを取得し、BeautifulSoupオブジェクトを返す\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[ERROR] Failed to fetch: {url}\")\n",
    "        print(e)\n",
    "        return None\n",
    "    return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "def extract_facility_urls(ranking_url):\n",
    "    \"\"\"\n",
    "    ランキングページから 1位〜100位 に該当する温泉施設のURLを一覧で取得する。\n",
    "    \"\"\"\n",
    "    soup = fetch_page(ranking_url)\n",
    "    if not soup:\n",
    "        return []\n",
    "\n",
    "    # liタグで classに normal, small, mini が付いているものを抽出 (1〜100位)\n",
    "    facility_li_list = soup.select(\"li.normal, li.small, li.mini\")\n",
    "\n",
    "    facility_urls = []\n",
    "    for li in facility_li_list:\n",
    "        a_tag = li.select_one(\"a[href]\")\n",
    "        if a_tag:\n",
    "            href = a_tag.get(\"href\")\n",
    "            # onsen.nifty.comでは href はフルURLなのでそのまま格納\n",
    "            if href.startswith(\"http\"):\n",
    "                facility_urls.append(href)\n",
    "            else:\n",
    "                # 万が一相対URLだったらフルURLに変換\n",
    "                facility_urls.append(\"https://onsen.nifty.com\" + href)\n",
    "\n",
    "    # 重複排除してから先頭100件だけに絞る\n",
    "    facility_urls = list(dict.fromkeys(facility_urls))\n",
    "    return facility_urls[:100]\n",
    "\n",
    "def scrape_facility_details(facility_url):\n",
    "    \"\"\"各温泉施設ページから詳細情報（効能など）を取得\"\"\"\n",
    "    print(f\"Fetching details from: {facility_url}\")\n",
    "    soup = fetch_page(facility_url)\n",
    "    if not soup:\n",
    "        return {\n",
    "            \"name\": \"N/A\",\n",
    "            \"address\": \"N/A\",\n",
    "            \"tel\": \"N/A\",\n",
    "            \"features\": \"N/A\",\n",
    "            \"benefits\": \"N/A\",\n",
    "        }\n",
    "\n",
    "    def extract_text(selector):\n",
    "        \"\"\"soup.select_one(selector)からテキストを抜き出す簡易関数\"\"\"\n",
    "        element = soup.select_one(selector)\n",
    "        return element.text.strip() if element else \"N/A\"\n",
    "\n",
    "    # 以下のセレクタはサンプルです。実際のHTML構造によっては取得できない場合があるため要調整。\n",
    "    name = extract_text(\"th:contains('施設名') + td\")\n",
    "    address = extract_text(\"th:contains('住所') + td\")\n",
    "    tel = extract_text(\"th:contains('TEL') + td\")\n",
    "    features = extract_text(\"dt:contains('特徴') + dd\")\n",
    "    benefits = extract_text(\"dt:contains('効能') + dd\")\n",
    "\n",
    "    # 他にも「泉質」や「料金」など必要があれば同様の方法で抽出可能\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"address\": address,\n",
    "        \"tel\": tel,\n",
    "        \"features\": features,\n",
    "        \"benefits\": benefits,\n",
    "    }\n",
    "\n",
    "def save_to_csv(all_facilities):\n",
    "    \"\"\"取得した温泉データをCSVに保存\"\"\"\n",
    "    with open(OUTPUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\"name\", \"address\", \"tel\", \"features\", \"benefits\"]\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_facilities)\n",
    "\n",
    "def save_to_db(all_facilities):\n",
    "    \"\"\"取得した温泉データをSQLiteデータベースに格納\"\"\"\n",
    "    # SQLiteファイルに接続 (無ければ新規作成される)\n",
    "    conn = sqlite3.connect(\"onsen.db\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # テーブル作成 (なければ作る)\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS onsen_facilities (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT,\n",
    "            address TEXT,\n",
    "            tel TEXT,\n",
    "            features TEXT,\n",
    "            benefits TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # データをINSERT\n",
    "    for fac in all_facilities:\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO onsen_facilities (name, address, tel, features, benefits)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            fac[\"name\"],\n",
    "            fac[\"address\"],\n",
    "            fac[\"tel\"],\n",
    "            fac[\"features\"],\n",
    "            fac[\"benefits\"]\n",
    "        ))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"温泉情報を取得してCSV・DBに保存し、コンソールにも出力するメイン処理\"\"\"\n",
    "    # 1. ランキングページから温泉施設のURL一覧を取得\n",
    "    onsen_urls = extract_facility_urls(RANKING_PAGE_URL)\n",
    "\n",
    "    all_facilities = []\n",
    "\n",
    "    # 2. 各施設ページへアクセスして詳細情報を取得 (サーバー負荷軽減のためtime.sleep)\n",
    "    for url in onsen_urls:\n",
    "        details = scrape_facility_details(url)\n",
    "        all_facilities.append(details)\n",
    "        time.sleep(1)  # サーバー負荷を軽減するために1秒ウェイト\n",
    "\n",
    "    # 3. 抽出結果を表形式で表示\n",
    "    print(\"\\n--- スクレイピング結果（最大100件） ---\")\n",
    "    print(tabulate(all_facilities, headers=\"keys\", tablefmt=\"grid\"))\n",
    "\n",
    "    # 4. CSVに保存\n",
    "    save_to_csv(all_facilities)\n",
    "    print(f\"\\n[INFO] Saved {len(all_facilities)} facilities to '{OUTPUT_FILE}'\")\n",
    "\n",
    "    # 5. DBに保存\n",
    "    save_to_db(all_facilities)\n",
    "    print(f\"[INFO] Inserted {len(all_facilities)} records into 'onsen.db'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import re\n",
    "\n",
    "# ▼ Mac 環境でヒラギノ角ゴシックW4を指定する例\n",
    "fp = FontProperties(fname=\"/System/Library/Fonts/ヒラギノ角ゴシック W4.ttc\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 1) CSV読み込み\n",
    "# ---------------------------------\n",
    "df = pd.read_csv(\"onsen_details.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 2) 効能をカウントしてグラフ化\n",
    "# ---------------------------------\n",
    "benefit_counter = Counter()\n",
    "\n",
    "for benefits in df[\"benefits\"]:\n",
    "    if not isinstance(benefits, str) or benefits == \"N/A\":\n",
    "        continue\n",
    "    # 効能を \"、\" で分割\n",
    "    splitted = benefits.split(\"、\")\n",
    "    splitted = [b.strip() for b in splitted if b.strip()]\n",
    "    \n",
    "    benefit_counter.update(splitted)\n",
    "\n",
    "# 多い順にソート\n",
    "most_common_benefits = benefit_counter.most_common()\n",
    "\n",
    "# 上位10件をグラフ表示\n",
    "top_n = 10\n",
    "top_benefits = most_common_benefits[:top_n]\n",
    "benefit_names, benefit_counts = zip(*top_benefits)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(benefit_names, benefit_counts, color=\"skyblue\")\n",
    "\n",
    "plt.title(f\"Top {top_n} 効能 出現頻度\", fontproperties=fp, fontsize=14)\n",
    "plt.xlabel(\"効能\", fontproperties=fp, fontsize=12)\n",
    "plt.ylabel(\"出現回数\", fontproperties=fp, fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\", fontproperties=fp)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# すべての効能＆カウントをテーブル表示\n",
    "all_benefits_df = pd.DataFrame(most_common_benefits, columns=[\"benefit\", \"count\"])\n",
    "display(all_benefits_df)\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# 3) どの都道府県に温泉が多いかを集計\n",
    "# ---------------------------------\n",
    "\n",
    "# 都道府県の抜き出し用の簡易関数 (正規表現で「北海道」「東京都」「大阪府」「京都府」 or 「何々県」を拾う)\n",
    "def parse_prefecture(address: str):\n",
    "    if not isinstance(address, str):\n",
    "        return \"不明\"\n",
    "    # 北海道のように「道」が含まれない場合\n",
    "    if address.startswith(\"北海道\"):\n",
    "        return \"北海道\"\n",
    "    \n",
    "    # 東京都/大阪府/京都府 を含むか、XX県 を含むかを正規表現で検索\n",
    "    match = re.search(r\"(東京都|大阪府|京都府|.{1,3}県)\", address)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"不明\"\n",
    "\n",
    "pref_counter = Counter()\n",
    "\n",
    "for addr in df[\"address\"]:\n",
    "    if isinstance(addr, str):\n",
    "        pref = parse_prefecture(addr)\n",
    "        if pref != \"不明\":\n",
    "            pref_counter[pref] += 1\n",
    "\n",
    "# 多い順に並べ替え\n",
    "most_common_pref = pref_counter.most_common()\n",
    "\n",
    "# 上位10件をグラフ表示\n",
    "top_n_pref = 10\n",
    "top_pref = most_common_pref[:top_n_pref]\n",
    "pref_names, pref_counts = zip(*top_pref)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(pref_names, pref_counts, color=\"orange\")\n",
    "\n",
    "plt.title(f\"温泉施設数が多い都道府県トップ{top_n_pref}\", fontproperties=fp, fontsize=14)\n",
    "plt.xlabel(\"都道府県\", fontproperties=fp, fontsize=12)\n",
    "plt.ylabel(\"施設数\", fontproperties=fp, fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\", fontproperties=fp)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# すべての都道府県＆施設数をテーブル表示\n",
    "all_pref_df = pd.DataFrame(most_common_pref, columns=[\"prefecture\", \"count\"])\n",
    "display(all_pref_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
